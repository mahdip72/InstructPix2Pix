use_ema: True
revision: null
non_ema_revision: null
enable_xformers_memory_efficient_attention: True
freeze_text_encoder: True
gradient_checkpointing: True
seed: 42
logging_dir: save_results
checkpoints_total_limit: 1 # did not work
checkpointing_steps: 2500
tensorboard: True
pretrained_model_name_or_path:  timbrooks/instruct-pix2pix  # runwayml/stable-diffusion-v1-5, stabilityai/stable-diffusion-2-1, timbrooks/instruct-pix2pix  #Path to pretrained model or model identifier from huggingface.co/models.
adapt_unet: False # True or False

resume_from_checkpoint:
  enable: False
  accelerator_path: /resume/to/checkpoint/save_results/2020-20-20__20-20-20/accelerator_checkpoints/checkpoint-2000

train_settings:
  data_dir: /mnt/hdd8/mehdi/datasets/instruct_pix2pix/open_image/images
  swap_input_output: False
  allow_tf32: False
  mixed_precision: "fp16" # ["no", "fp16", "bf16"]
  batch_size: 8
  gradient_accumulation_steps: 16
  max_train_steps: 10000
  num_workers: 2
  shuffle: True
  final_resolution: 512
  conditioning_dropout_prob: 0.05
  min_snr_loss:
    enable: True
    snr_gamma: 5.0


valid_settings:
  data_dir: /test_images # A folder containing images
  num_images: 5
  epoch: 20
  prompt_list: ['make the sky yellow']
  resolution: 512
  num_inference_steps: 30
  image_guidance_scale: 1.5
  guidance_scale: 7.5

optimizer:
  learning_rate: 5e-5  # Initial learning rate (after the potential warmup period) to use.
  scale_lr: False # Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.
  lr_scheduler: cosine  # Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"]
  lr_warmup_steps: 300 # Number of steps for the warmup in the lr scheduler.
  use_8bit_adam: True # Whether to use 8-bit Adam from bitsandbytes.
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_weight_decay: 1e-2
  adam_epsilon: 1e-08
  max_grad_norm: 1.0

augmentations:
  rand_horizontal_flip: True
  crop_resolution: 768
  use_target: False
  rotation: False

